\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Implementation}
\section{The Technology Stack}
\subsection{The Language Server}
Before any work could be done, I needed to decide on which technology stack to use. This was dictated mainly by the programming language involved. The following features were needed:
\\
\\
\textbf{Linux, Windows and Mac OS support} \\
The language server had to be able to connect to offline editors, and therefore run on user's desktops. This meant that the most up-to-date editions of the three most popular operating systems -- Linux, Windows and Mac OS X -- had to be supported.
\\
\\
\textbf{Support for strong typing} \\
Since creating a language server is a large and complex project, I needed to be using a langauge with strong typing in order to both avoid mistakes and receive context-aware support from my IDE.
\\
\\
\textbf{A Language Server Protocol API} \\
Writing Language Server Protocol requests manually would be inefficient, time-consuming and a potential cause of errors. A library that abstracted away the exact layout and content of Language Server Protocol requests would aid productivity.
\\ 
\\
These last two requirements only left two libraries: the \\ 
\texttt{Microsoft.VisualStudio.LanguageServer}, written for C\# \cite{visual_studio_language_server}, or \\ 
\texttt{vscode-languageserver}, written for TypeScript \cite{vsc_langserver_docs}. Since these libraries are were created by Microsoft \footnote{This should not be surprising, as Microsoft also created the Language Server Protocol.}, they are structured quite similarly. 
\\
\\
In the end, the TypeScript library was chosen over the C\# library. Both libraries being quite similar, this decision was made because TypeScript was better suited for the task than C\#. The Language Server Protocol communicates using JSON, and TypeScript can handle JSON more fluidly than C\# can. Useful features include destructuring JSON, giving JSON objects unique types based on their fields, and treating JSON objects as implementing interfaces that have the same fields. This decision being made, the resulting technology stack was TypeScript run locally using \codeword{Node.JS}. 

\subsection{The Syntax Highlighter}
Since the syntax highlighting is specified in a single JSON document, the technology stack required was minimal. Rather than writing the JSON document directly, I chose to write the TextMate grammar in a YAML document, from which the JSON document was then automatically generated using the command \codeword{yq} \cite{yq_repo}. This was done because of the length of the JSON required: YAML documents are easier to read due to their less cluttered syntax, with the scope of objects being determined by whitespace rather than brackets. Writing regular expressions is also easier in a YAML document. Regular expressions feature many backslashes: in JSON, unlike in YAML, these backslashes need to be escaped with another backslash. Regular expressions are confusing enough to read as they are -- I did not need any added confusion by having to parse escaped backslashes in my head!

\subsection{The Language Client}
Language servers written using the \texttt{vscode-languageserver} library connect seamlessly to visual studio code language clients written using the \texttt{vscode-languageclient} package. Thus, my main method of day-to-day testing was done using a local visual studio code client. I could be assured that all errors I found were due to the language server itself, not the connection, since the two libraries were built with each other in mind. Tests were also done using a Codemirror 5 client that ran locally in the browser, to see which features carried over to Codemirror 5.



\section{The Syntax Highlighter}
Using TextMate grammar, the syntax highlighter identified three tiers of Logical English features.

\subsection{Sub-Line Features}
The syntax highlighter marks keywords that are contained within a single line, such as \codeword{if}, \codeword{and}, \codeword{or}, \codeword{it is the case that} and \codeword{it is not the case that}. It also marks the pre-defined constant term \codeword{unknown}. This is done simply by recognising these keywords wherever they appear and are surrounded by word boundaries (i.e. spaces, tabs or other non-word characters). 
\\ 
\\ 
A proposed feature is for the language extension to allow for such keywords to appear in template names, where they would have to be marked differently. This would not be a problem: the language server can override the syntax highlighter in its semantic highlighting.
\\ 
\\ 
The syntax highlighter also marks single-line comments that begin with \codeword{\%} and span until the end of the line. However, all the syntax highlighter does is mark them: it is a separate feature of the language server to ignore comments.

\subsection{Single-Line features}
The syntax highlighter also marks the headers of sections of a Logical English document. For the template section header, \codeword{the templates are:}, the characters from the initial \codeword{t} up to the final \codeword{e} is marked as a single header block. Beginning at the initial word character is an important point, since Logical English allows headers to be indented. 
\\ 
\\\
The knowledge base, scenario and query headers all support being named. Thus, their names are marked separately from the rest of the header. 
\\ 
\\
It is interesting to note what TextMate grammar identifiers are used to mark the headers. Following the TextMate naming conventions \cite{textmate_grammars_spec} is essential to maximise the amount of colour schemes that colour Logical English documents correctly. However, the guidelines are quite vague, with \codeword{entity}, \codeword{meta} and \codeword{markup.heading} all being recommended for use in marking up section headers. 
\\ 
\\
The most pragmatic course of action was to survey a variety of popular TextMate grammars. Finding analogies of section headers in popular languages was difficult. In C-style languages, sections of code are either labelled with single-word keywords such as \codeword{if}, \codeword{for} or \codeword{while}, or also name a type of data structure, such as class or interface names. These are semantically quite different from Logical English section headers. In the end, I settled on a function name, used in a function definition or declaration, as being the closest analogy. Although the function name may reappear in the document, it does not represent a type and rarely represents data (the data instead usually being the return value, at the end of a function call). This was usually labelled with \codeword{entity.name} prefix. The fact that the names of HTML tags are also labelled with \codeword{entity.name} was further evidence that \codeword{entity.name} was the best choice. 
\todo[inline]{Put some references to popular IDE's default TextMate schemes here. Also maybe talk about why the name of a query / section / knowledge base is given variable.}


\section{The Language Server}
The language server is a complex tool and has multiple features. Each category of feature corresponds, in the most part, been extracted into a single source code file. 


\subsection{General Design choices}
\todo[inline]{Research design methodologies, evaluate them for this problem. Cite which methodology corresponds to mine.}


\subsection{Parsing the document}
\todo[inline]{Talk about how different parts of the document: sections, templates, clauses, literals are extracted. Talk about ContentRanges.}


\subsection{Template Functionality}
\todo[inline]{Rewrite this to be about templates only.}
In parsing a Logical English document, the language server is heavily reliant on ``helper functions". A common class of problem is, given a pattern or structure, to extract parts of a block of code according to the pattern. For instance, is done in (but is not limited to):
\begin{enumerate}
    \item extracting the template section or knowledge base section from the document
    \item extracting clauses from the knowledge base section, and templates from the template section
    \item extracting literals from clauses
    \item extracting terms from a literal, assuming that a literal matches a given template
    \item generating a template from a literal, given the literal's terms
    \item determining whether a literal matches a given template
\end{enumerate}
These core functionalities are used throughout the language server, so it is worth spending some time to understand how these functionalities work. The first three can be done syntactically: these are essentially done through simple regular expressions. The last three are more complex.

\subsubsection{Representing a template}
Before I could solve the two problems, I first had to design a suitable data structure for representing a template. The more effectively the structure is designed, the easier the solutions will be; in fact, the Template class proved to be the backbone of the language server. Such a design must be close enough to the conceptual notion of a template to be powerful, whilst also being close enough to the Logical English syntax of a template for it to be applicable to the problem. 
\todo{Talk about my intial design and why I changed it.}
\\ 
\\
Conceptually, a template is a list \todo{think of a better word} consisting of either variables, or text that makes up the template's name. Thus a natural representation is a wrapper class around an array of \codeword{TemplateElement} items, where a \codeword{TemplateElement} is either a \codeword{TemplateVariable} or a \codeword{PredicateWord}. Expressed in TypeScript:

\begin{lstlisting}
    type TemplateElement = TemplateVariable | PredicateWord;

    class Template {
        private readonly elements: TemplateElement[];
        ...
    }
\end{lstlisting}
The \codeword{TemplateVariable} and \codeword{PredicateWord} types are classes that contain their text label. Both also contain a \codeword{type} field, that specifies which of the two types the object is. This way, any \codeword{TemplateElement} can have its \codeword{type} field queried. \\
Expressed in TypeScript:

\begin{lstlisting}
    enum TemplateElementKind { 
        Variable,
        Word
    }

    class TemplateVariable {
        public readonly name: string;
        public readonly type = TemplateElementKind.Variable;
        ...
    }

    class PredicateWord {
        public readonly word: string;
        public readonly type = TemplateElementKind.Word;
        ...
    }
\end{lstlisting}
This design choice allows \codeword{TemplateVariable} to have extra functionality added without breaking the existing code. The creators of Logical English are considering to give types to template variables and perform type checking at runtime to find mistakes in user's Logical English code. This could easily be accommodated by this current design by simply adding extra fields and methods to the \codeword{TemplateVariable} class.
\todo[inline]{Talk a bit more about how an actual string is converted using regex.}


\subsubsection{Using a template to extract terms from a literal}
Extracting the terms from a literal, assuming that the literal matches a given template, is done using a state engine. The (space-separated) words in the literal are iterated over. Each word \codeword{w} is compared to the template's first predicate word. \\ 
If they do not match, \codeword{w} must be part of a term, so \codeword{w} is appended to a string buffer. When \codeword{w} matches the first predicate word, we have reached the end of the term, so the buffer is appended to a list of terms and is cleared.
\todo[inline]{Make this into a diagram or pseudocode.}
\todo[inline]{Research into state engine.} 


\subsubsection{Generating a template from a literal}
Given a literal and a list of the literal's terms, it is fairly straightforward to generalise the literal into a corresponding template. This is done by leveraging the fact that a template's elements alternate between being a variable and being a predicate word. In other words, the literal's predicate words are each separated by exactly one term. This means that we can obtain the literals elements by splitting the literal, with each of the literal's terms as delimeters. Doing so, using TypeScript's powerful regular expressions system, yields a list of the template's elements, in string form. From this, the corresponding \codeword{TemplateElement[]} is generated by checking which elements are terms and which must be predicate words. The template is then built from this array.

\todo[inline]{Think of a more succint name than ``predicate word''.}
\todo[inline]{Rewrite this using a diagram or pseudocode.}

\subsubsection{Determining whether a literal matches a template}
The above two functionalities are put together to determine whether a literal $L$ matches a given template $T$. I first assume that $L$ does indeed match $T$, and extract $L$'s terms. Using $L$'s terms, I generalise $L$ into a new template $T^\prime$. It then suffices to check whether $T$ and $T^\prime$ are isomorphic: that is, whether $T$'s variables could be renamed to give $T^\prime$.
\todo[inline]{Write this as a formal proof.}

\subsection{Genreralising two or more literals into a shared template}
Talk about Least General Generalisation.

\subsection{Connection to a Language Client}
\todo[inline]{Research more about how this works.}


\subsection{Semantic Highlighting}
The language server augments the syntax highlighter by highlighting the terms that appear in literals. This is a step beyond the regular expressions of the syntax highlighter. To do this, the server must understand the structure of each template, which template each literal is an instance of, and how to use this matching to identify which substrings of a literal are terms. Each of these functionalities are described in the previous section: here, I will explain how they are put together.
\\ 
\\
The server first extracts the template and literal strings in the document. Each template string is converted into an instance of the \codeword{Template} class.
The semantic highlighter finds the first template that matches each literal. 
\todo[inline]{This could lead to bugs. Instead, find the most specific template that matches the literal. Code this up then talk about why this is necessary, and how specificity is determined.} It uses this template to extract the literal's terms. The position of each term in the document is calculated using the \codeword{ContentRange} of each literal, offset by the position of each term in the literal. At first, I naively used the \codeword{String.indexOf} function to find the character positions of each term. When the same term occured more than once, the position of the first term was given both times, which lead to subsequent repitions of the term not being highlithed. Instead, the line position of the previous term was added to the...
\todo[inline]{Needlessly complicated. Rewrite code to use indexOf(element, startingIndex)instead.} 


\subsection{Code Completion}
Here I'll explain how half-written literals are matched against templates. I'll explain Template.matchScore, how half-written literals can have their existing terms extracted and substituted into their template. 

\subsection{Error Diagnostics}
Here I'll explain how literals are iterated over and checked whether they match any template. ContentRange is used to work out where the error diagnostic should go.  \\ 
I'll also talk about how clauses are checked for misaligned connectives. ContentRange is used to work out where error diagnostic should go.

\subsection{Error Fixes}
Explain how the literals without any template are re-calculated, and talk about how this is inefficient but necessary by my stateless design. May optimise later. 
\\ 
\\ 
Talk about how the least general generalisation of all template-less literals is calculated. Why it is a good idea in practice to generate a template from all such literals -- users will start with code where all literals match and introduce new literals that dont match. 
\\ 
\\ 
Here I should also talk about doing more than just LGG. If Im in a clause where literal A has T as a term, and I write a literal B that uses T, then when generalising B I should replace T with a varaible.


\section{The Visual Studio Code Language Client}
\todo[inline]{Research how connection works}

\section{The Codemirror Language Client}
Talk about how it connects to language server via a stdio to websocket proxy.

\end{document}
